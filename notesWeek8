11/19/24
Naive Bayes
Simple idea:
Assume all probabilities are independent
Model relationship between each feature and class
Use Bayes' rule to compute overall class probability

Key design decisions:
Representing probabilistic decisions for diff kinds of features
Smoothing (using prior probabilities) to avoid overfitting

Goal: compute posterior over Y, F
Step 1: Get joion probability of Y (class) and F (evidence)
Step 2: Get probability of evidence
Step 3: Renormalize - In many cases this can be skipped

Inference:
	Start with bunch of conditionals, P(Y) and the P(Fi|Y) tables
	Use standard inference to compute P(Y|F1...Fn)

Estimation of local conditional probability tables:
	P(Y) the prior over labels
	P(Fi|Y) for each feature (evidence variable)
	This data typically comes from training data

For a number detector, for each individual pixel the conditional probability will be computed

Naive Bayes for Text
Bag-of-Words Naive Bayes
Predict unknown class label (spam vs ham)
Assume evidence features (words) are independent
No longer care about order of words, just care about the words that are used

Generative model
	P(C, W1 ... Wn) + P(C) * product (P(Wi|C)) - Word at position i, not ith word in the dictionary

To get prior probability of a given word
P(ci) = #ci / N - # means count the number of times ci is seen
P(fi|ci) = (#ci, fi) / #ci - Number of times ci appears given fi is the feature
Probability of a word given a feature = number of times the word shows up with the given feature / number of times that feature appears

One thing that can go wrong with Naive Bayes is overfitting
If something shows up that wasn't in training data then one of the probabilities for a feature will become 0 and it will multiply everything else to make the probability 0

Can't just assume probability of unseen events is zero
Can use smoothing or regularizing to make estimates better

Smoothing
Maximum likelihood estimates:
	P_ML(x) = count(x) / total samples
Basic idea:
	We have some prior expectation about parameters (e.g probability of heads)
	Given little evidence we should skew towards prior
	Given a lot of evidence, we should listen to data

Estimation: Laplace Smoothing
Pretend you saw every outcome k extra times
Works with conditional probabilities
P_LAPk(x) = (c(x) + k) / (N + k|X|)
k is the strength of the prior
|X| is size of outcomes of x
	When flipping a coin you either get heads or tails, so this is 2

Given H H T when flipping 3 coins
Laplace with 0 = <2/3, 1/3>
Laplace with 1 = <3/5, 2/5>
Laplace with 100 = <102/203, 101/203>

Common way to do laplace transform
Just do laplace with 1
This ensures that you have no zero probabilities


Recap Linear Models
General framework for binary classification
Cast learning as an optimization problem
Optimization objective combines two terms
	Loss function: measures how well classifier fits training data
	Regularizers: measures how simple classifier is
Does not assume data is linearly separable

Model Evaluation
How do we choose the best model?

Model Complexity
	Simple models have fewer paraters ... and have less complex decision boundaries ... but may not be able to fit data as well
Models from least to most complex
	average/mode - Kinda good to use this as a baseline. If your model can't beat this, it is not good
	linear models
	decision trees
	nearest neighbor
	naive bayes
	neural networks

Overfitting
Consider a hypothesis h and its:
Error ate over training data error_train(h)
True error rate over all data error_true(h)

We say h overfits the training data if error_train(h) < error_true(h)
Amount of overfitting = error_true(h) - error_train(h)
Problem: we don't know error_true(h)
Solution
	We set aside a test set
		Some examples that will be used for evaluation
	Don't look at it during training
	After learning a model we calculate error_test(h)

We have two kinds of unknowns
Parameters: weight in model, decision tree splits, etc.
Hyperparameters: like amount of regularization to do: k, etc.

How to learn?
Learn parameters from training data
Tune hyperparameters on diff data
For each value of hyperparameters train on held out data
Choose best value to do final test on data

There is randomness in how test set is selected, what if you get unlucky and particular test set doesn't work well for certain model even though across total population it is better
Good to do random selection of data, because it may have been sorted ahead of time

K-Fold Cross Validation
Splite data into k equally sized splits
For each split, use it as test data, while using other k-1 splits as training data
Compute average and standard deviation of performance metric (accuracy, F1, etc)

Can compare different models doing this, as long as they are using the same splits
Most common method used is called a paired t-test
Another way to do this is with a box plot
	Shows minimum, first quartile, median, third quartile, and maximum


11/21/24
Nearest Neighbor
	Store all training examples
	Classify new examples based on finding most similar training examples

Two approaches to learning
Eager Learning
	Learn/Train
		Induce an abstract model from data
	Test/Predict/Classify
		Apply learned model to new data
Lazy Learn
	Learn
		Just store data in memory
	Test/Predict/Classify
		Compare new data to stored data
	Can represent complex hypothesis space
	Classification can be very slow

Components of k-NN classifier
Distance metric
	How do we measure similarity between instances
K hyperparameter
	How many other data points should you look at
	Affects complexity of hypothesis space
	If k = 1 then every training example has its own neighborhood
	If k = N, entire feature space is one neighborhood - Averaging more and more

Distance metrics
L2 distance - Euclidean distance
	Circles
L1 distance - manhattan distance
	Squares tilted at 45 degree angle
L infinity distance - chebyshev distance
	Squares

Default: all neighors have equal weight
Extension: weight neighbors by (inverse) distance

Epsilon Ball Nearest Neighbors
Instead of specifying how many neighbors to consider, just takes all neighbors that are within a certain distance


Decision Trees
Inputs - Discrete or continuous
Outputs - discrete (classification) or continuous (regression)
Easier for humans to look at and understand

Hypothesis space grows for these very quickly if trying to represent all possible outcomes
Simpler is better, generally for tree - Ockham's razor
	Avoids over-fitting

Greedy Algorithms
Like many NP-complete problems we can get pretty good solutions
Most decision tree learning is greedy
Start out and make best local decisions and keep trying to make best local decisions
Do this by top-down decision tree learning

if all labels are same
	return leaf node for that label
else
	let f be best feature for splitting
	left = BuildDecisionTree(data, with f=0, labels with f=0)
	right = BuildDecisionTree(data with f=1, labels with f=1)
	return Tree(f, left, right)

Does this always terminate? - Yes

Other base cases
All data have same label
	Return that label
No examples left
	Return majority label of all data (mode of data)
No further splits possible
	Return majority label of passed data
Good attribute splits the examples into subsets that are (ideally) "all positive" or "all negative"

How to measure information?
	Information theory

Information theory
	Quantification of information
Entropy - H(Y)
In information theory, entropy is measure of uncertainty associated with random value
Quantifies, using expected value, information contained in a message in bits
Measure of average information content one is missing when one does not know the value of the random variable
One interpretation of entropy is the expected number of bits needed to encode Y or number of questiones needed to guess Y

Information Gain
To quantify predictiveness of attribute X for Y, we consider the conditional entropy, or expected number of bits needed to enocde Y or questiones needed to guess Y, knowing X

Random Forests
Makes multiple decision trees and combines the total results
Good model for large number of features
Generally provide high accuracy
Not easily interpretable
Computationally intensive for large datasets
